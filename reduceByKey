from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("pysparktest").setMaster("local[*]")
sc = SparkContext(conf=conf)

data = ["Project", "Gutenberg's", "Alice's", "Adventures",
        "in", "Wonderland", "Project", "Gutenberg's", "Adventures",
        "in", "Wonderland", "Project", "Gutenberg's"]

rdd = sc.parallelize(data)

rdd2 = rdd.map(lambda x: (x, 1))
rdd3 = rdd2.reduceByKey(lambda a,b: a+b)
for element in rdd3.collect():
    print(element)
