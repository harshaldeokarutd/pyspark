from pyspark.sql import SparkSession


spark = SparkSession.builder.master("local[1]") \
    .appName('SparkByExamples.com') \
    .getOrCreate()

rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7], 4)
print(len(rdd.repartition(2).glom().collect()))
