from pyspark.sql import SparkSession


spark = SparkSession.builder.master("local[1]") \
    .appName('SparkByExamples.com') \
    .getOrCreate()

rdd = spark.sparkContext.parallelize([1, 2, 3, 4], 4)


def f(splitIndex, iterator): yield splitIndex


print(rdd.mapPartitionsWithIndex(f).sum())
